[
  {
    "objectID": "daily.html",
    "href": "daily.html",
    "title": "Today I Learned",
    "section": "",
    "text": "Title\n\n\nDate\n\n\n\n\n\n\n\n \n\n\n\nFatal: Not possible to fast-forward, aborting\n\n\nSep 17, 2022\n\n\n\n\n\n\n\nlabelled vs labeled\n\n\nMay 13, 2022\n\n\n\n\n\n\n\nConnecting two Indpendent Clauses with Comma and Semicolons\n\n\nMay 11, 2022\n\n\n\n\n\n\n\nBias and Varience in Machine Learning Algorithms\n\n\nMay 10, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "bookreading.html",
    "href": "bookreading.html",
    "title": "Book I am reading",
    "section": "",
    "text": "Title\n\n\nDate\n\n\n\n\n\n\n\n\n\nDeep Work\n\n\nOct 15, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "dailyml/dailyml0001/dailyml_0022.html",
    "href": "dailyml/dailyml0001/dailyml_0022.html",
    "title": "Active Learning",
    "section": "",
    "text": "Claire and Phil aren’t on the same page with their plan.\nThey want to train a machine learning model but want to minimize the number of samples they need to label. Labeling takes too long, and they want to avoid it as much as possible.\nClaire argues that they don’t need to train with the entire dataset. Instead, she believes they can maximize the model’s performance without using all the data.\nPhil disagrees. He argues that the only way to achieve the maximum possible performance is to train with the entire dataset. Since they aren’t willing to label all the data, they will need to settle for a mediocre model.\nWhat’s your opinion about this situation?\n\nAchieving the maximum possible performance without using the entire dataset is theoretically possible but very unlikely.\nThey can achieve the maximum possible performance without using the entire dataset by randomly sampling a portion of the data, labeling it, and training the model.\nThey can achieve the maximum possible performance without using the entire dataset, but they need a good strategy to sample the data they will label to train the model.\nThey will never achieve the maximum possible performance without using the entire dataset.\n\n\n\n\nWith active learning, we can build a model that will achieve better performance with fewer labeled samples by allowing the algorithm to choose the data that will provide the most information to its training process.\n\nClaire and Phil do not need to use the entire dataset to build a model that reaches its maximum possible performance. However, they will need a smart strategy to select the data they need to label.\nLet’s imagine a dataset with two classes that we can represent in two dimensions and a linear model that splits the data into two groups. Any samples around the lines’ boundaries that separate both classes are critical in our dataset. Those samples help the model decide how to split the data!\nBut what about samples far away from the split? They contribute much less to the model, and we don’t need them to find the separation between classes. The same happens with duplicate samples or samples that are too similar to existing ones.\nClaire and Phil, however, can’t depend on randomly sampling the dataset to decide which instances to label. They need a better strategy to determine which samples to pick.\nThis scenario is an example of Active learning. This learning technique allows us to build a better-performing machine learning model using fewer training labels by strategically choosing the samples to train the model.\n\n\n\n\nActive learning\nA Short Introduction to Active Learning\nTutorial on Active learning"
  },
  {
    "objectID": "til.html",
    "href": "til.html",
    "title": "Today I Learned",
    "section": "",
    "text": "Title\n\n\nDate\n\n\n\n\n\n\n\n \n\n\n\nFatal: Not possible to fast-forward, aborting\n\n\nSep 17, 2022\n\n\n\n\n\n\n\nlabelled vs labeled\n\n\nMay 13, 2022\n\n\n\n\n\n\n\nConnecting two Indpendent Clauses with Comma and Semicolons\n\n\nMay 11, 2022\n\n\n\n\n\n\n\nBias and Varience in Machine Learning Algorithms\n\n\nMay 10, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "dailyR.html",
    "href": "dailyR.html",
    "title": "Daily R Practice",
    "section": "",
    "text": "Title\n\n\nDate\n\n\n\n\n\n\n\n \n\n\n\nR4DS Chapter 2 Exercises\n\n\nOct 15, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Quotes.html",
    "href": "Quotes.html",
    "title": "Shamsuddeen Hassan Muhammad's Blog",
    "section": "",
    "text": "In this page, I documents Quotes that I found useful for my future reference.\n\n\n\n“One cannot think well, love well, sleep well, if one has not dined well.” ~Virginia Woolf “Keep learning, or risk becoming irrelevant.”"
  },
  {
    "objectID": "dailyml.html",
    "href": "dailyml.html",
    "title": "Daily Machine Learning",
    "section": "",
    "text": "Title\n\n\nDate\n\n\n\n\n\n\n\n \n\n\n\nActive Learning\n\n\nOct 15, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "NLP.html",
    "href": "NLP.html",
    "title": "Shamsuddeen Hassan Muhammad's Blog",
    "section": "",
    "text": "These are collections of stuff related to NLP that I came across and\n\n\n1.DocNow: Is a tool for appraising, collecting, and gathering consent for Twitter content.\n\nTwarc: Twarc is a command line tool and Python library for collecting tweet data from Twitter’s official API. It is designed for reliably collecting historical as well as realtime data, and can be used as a software library in your own tools and applications.\nHydrator is a desktop application for turning Tweet ID datasets back into tweet data to use in your research. It has been designed to be a reliable option for researchers who want to use their workstation for long running hydration jobs.\nThe Catalog is a community-sourced clearinghouse for tweet identifier datasets. Sharing tweet ids is a practice that is encouraged by Twitter as a way to share research data without negatively affecting users ability to have their data deleted or hidden from the web. We welcome your contributions!\n\n\n\n\n\nAwesome-NLP\nDeep Learning for NLP\nPyTorch Tutorial for Deep Learning Researchers\nPyTorch-Tutorial\n\n\n\n\n\nNatural Language Processing with Transformer\nTransformers for Machine Learning: A Deep Dive\nDeep Learning for NLP and Speech Recognition\n\n\n\n\n\nMultilingual Natural Language Processing - http://demo.clab.cs.cmu.edu/11737fa20/\nCSE 704 - Applied Natural Language Processing and Computational Social Science - https://kennyjoseph.github.io/cse702\nNLP Course | For You by Lena Voita - https://lena-voita.github.io/nlp_course.html\nDavid Bamman’s Course on Applied NLP - https://github.com/dbamman/anlp19 - Course Page\nCMU NLP - http://phontron.com/class/nn4nlp2017/schedule.html\nOxford Deep Learning NLP - https://github.com/oxford-cs-deepnlp-2017/lectures\nProbabilistic NLP course - https://uva-slpl.github.io/nlp2/syllabus.html\nMachine Learning: Linguistic & Sequence Modeling - https://seq2class.github.io/\nIntroduction to Natural Language Processing by Jacob Eisenstein - https://github.com/jacobeisenstein/gt-nlp-class\nA Course on word embeddings, variants, and applications: http://people.ds.cam.ac.uk/iv250/esslli2018.html"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "A No-Nonsense Guides to Pytorch Tensor\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata science\n\n\nunix\n\n\n\n\nThis blog explain how to use command line tools to obtain data from the internet.\n\n\n\n\n\n\nJun 1, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata science\n\n\nunix\n\n\n\n\nThis blog explain how to copy files from host to docker container and vice versa.\n\n\n\n\n\n\nJun 1, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nBooks\n\n\n\n\nWelcome to my Quarto Blog\n\n\n\n\n\n\nMay 31, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "datascience.html",
    "href": "datascience.html",
    "title": "Shamsuddeen Hassan Muhammad's Blog",
    "section": "",
    "text": "DevOps for Data Science"
  },
  {
    "objectID": "dailypython.html",
    "href": "dailypython.html",
    "title": "Daily Python Practice",
    "section": "",
    "text": "Title\n\n\nDate\n\n\n\n\n\n\n\n \n\n\n\nGetting and Knowing your Data\n\n\nOct 15, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "dailypaper.html",
    "href": "dailypaper.html",
    "title": "Daily paper summary",
    "section": "",
    "text": "Title\n\n\nDate\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/obtaining data/obtaininda.html",
    "href": "blog/obtaining data/obtaininda.html",
    "title": "Using command line tool to obtain data",
    "section": "",
    "text": "Data can be obtained in several ways—for example by downloading it from a server, querying a database, or connecting to a Web API. Sometimes, the data comes in a compressed form or in a binary format such as a Microsoft Excel Spreadsheet. Tools such as curl, tar, wget, csvtool and others can be used to download data. —Data Science at command Line"
  },
  {
    "objectID": "blog/obtaining data/obtaininda.html#downloading-from-the-internet-using-curl",
    "href": "blog/obtaining data/obtaininda.html#downloading-from-the-internet-using-curl",
    "title": "Using command line tool to obtain data",
    "section": "Downloading from the Internet using Curl",
    "text": "Downloading from the Internet using Curl\nCurl and wget are command line tools that allows you to download files from the Internet. curl and wget are installed by default on most Linux systems. if you do not have either tool installed, you can use the commands below to install curl and wget on Ubuntu.\n   sudo apt install curl\n   sudo apt install wget\nIf you are using a Mac, you can install curl and wget using the following commands:\n    brew install curl\n    brew install wget\n\nHow to Use Curl\nThe curl syntax is :\ncurl [options] [URL...] \nIn its simpform, curl is used to download a file from the Internet without any options.\n    curl example.com\nThe command above will download and print the source code of the example.com homepage in your terminal window.\n\n\nSave the Output to a File\nRather than displaying to the standard output, we can use the -o option to save the output to a file.\nThe above example will save the output to a file named example.html instead of printing it to the terminal.\ncurl -o example.html example.com # \nOR\ncurl http://example.com --output filename\nOR\n\ncurl example.com -o example.html\nYou can also redirect the output to a file using the > operator.\n    curl -o example.com > example.html\nSometimes, it’s a good idea to set the verbose mode on using -V option. This may provide useful information about the progress of the download.\n\ncurl -v example.com\ncurl outputs a progress meter that shows the download rate and the expected time of completion. You can also use the -s option to hide the progress meter and the progress.\ncurl -s example.com\n\n\nFollowing Redirects\nWhen accessing shortened URL with curl, such as: http://bit.ly/, we need to set the -L option to automatically follow redirects\n    curl -L http://bit.ly/\n\n\nUsing Curl to Download from FTP server\nCurl has over 20 built-in FTP commands. You can use curl to download files from FTP servers as follows:\ncurl -s \"ftp://ftp.gnu.org/welcome.msg\" | trim\n\n\nHow to use Wget Command\nThe wget command is similar to curl, but it does not print the output to the terminal. Instead, it saves the output to a file. Its syntax is shown below:\nwget [options] [URL...]\nYou can download a file from the Internet using wget and saved to a file with same name.\nwget example.com\nYou can also save the output to a diffrent file name using the -O option.\nwget -O example.html example.com\nYou can save in a specify folder using the -P option.\n    wget -P /home/example.com example.com"
  },
  {
    "objectID": "blog/obtaining data/obtaininda.html#difference-between-wget-and-curl",
    "href": "blog/obtaining data/obtaininda.html#difference-between-wget-and-curl",
    "title": "Using command line tool to obtain data",
    "section": "Difference between Wget and Curl",
    "text": "Difference between Wget and Curl\nWget is similar to Curl, but curl is more powerful. wget is a tool to download files from servers while curl is a tool that let’s you exchange requests/responses with a server.\nwget’s major advantage is its ability to download recursively. Wget is command line only. There’s no library.\nCurl is powered by libcurl - a cross-platform library with a stable API that can be used by each and everyone.\nCurl is generally preferred since it supports more features than wget. For comparison here is the list of features of available with different tools:"
  },
  {
    "objectID": "blog/obtaining data/obtaininda.html#other-cool-stuff-you-can-do-with-curl",
    "href": "blog/obtaining data/obtaininda.html#other-cool-stuff-you-can-do-with-curl",
    "title": "Using command line tool to obtain data",
    "section": "Other cool stuff you can do with curl",
    "text": "Other cool stuff you can do with curl\n\n\n\n\n\n\nGet weather information with : wttr.in\n\n\n\nYou can use curl to check weather info from your terminal\n\n\n!wttr.in/SanFrancisco # to check weather in San Francisco\nrate.sx: Crypto prices without leaving the terminal by running curl rate.sx\n\ncurl rate.sx\ndict.org: To look up definitions for a word, run curl ‘dict.org/d:word’\ncurl 'dict.org/d:word'"
  },
  {
    "objectID": "blog/obtaining data/obtaininda.html#compressing-and-decompressing-files",
    "href": "blog/obtaining data/obtaininda.html#compressing-and-decompressing-files",
    "title": "Using command line tool to obtain data",
    "section": "Compressing and Decompressing Files",
    "text": "Compressing and Decompressing Files\nLarge files can be compressed and decompressed.\n\nCompressing Files using tar\n    tar -cvf example.tar ~/desktop/example/\nThe options used in the above command to create a tar file are:\n\nc – Creates a new .tar archive file.\nv – Verbosely show the .tar file progress.\nf – File name type of the archive file.\n\n\n\nCompressing Files using gzip\ntar cvzf example.tar.gz /desktop/examples/\n\nor \n\ntar cvzf example.tgz /desktop/examples/"
  },
  {
    "objectID": "blog/obtaining data/obtaininda.html#list-content-of-an-archive-file",
    "href": "blog/obtaining data/obtaininda.html#list-content-of-an-archive-file",
    "title": "Using command line tool to obtain data",
    "section": "List Content of an Archive File",
    "text": "List Content of an Archive File\nTo list the contents of an archive file, you can use the following command (with t option):\n\ntar -tvf example.tar"
  },
  {
    "objectID": "blog/obtaining data/obtaininda.html#decompressing-archivedd-file",
    "href": "blog/obtaining data/obtaininda.html#decompressing-archivedd-file",
    "title": "Using command line tool to obtain data",
    "section": "Decompressing Archivedd File",
    "text": "Decompressing Archivedd File\nTo uncompress an archive file, you can use the following command (with x option):\n\ntar -xvf example.tar \nIf you want to untar in a different directory then use option as -C (specified directory).\n    tar -xvf example.tar -C /desktop/example/\nFinally, you can use unpack command also to decompress the file.\nunpack example.tar"
  },
  {
    "objectID": "blog/obtaining data/obtaininda.html#csvkit-converting-and-working-with-csv-at-command-line",
    "href": "blog/obtaining data/obtaininda.html#csvkit-converting-and-working-with-csv-at-command-line",
    "title": "Using command line tool to obtain data",
    "section": "Csvkit : converting and working with CSV at command Line",
    "text": "Csvkit : converting and working with CSV at command Line\ncsvkit is a suite of command-line tools for converting to and working with CSV, the king of tabular file formats. working with csvkit makes it easy to convert between different formats, and to work with the data in those formats.\nYou can install csvkit using the following command:\n    pip install csvkit\nWe can print the contents of the file using the following command:\nbat -A data.csv\nThe -A option show all non-printable characters such as space, tab, newline, etc.\nWe can also use another command called csvlook to print the contents of the file nicely.\ncsvlook data.csv\nBelow are some of the commands that you can use with csvkit:\n\nConvert Excel to CSV:\n\nin2csv data.xls > data.csv  # Excel to csv\n\nin2csv data.json > data.csv # json to csv\n\ncsvstat data.csv # give the statistics of the csv file\n\ncsvjson data.csv > data.json # convert csv to json\n\ncsvcut  -n data.csv # print column names\n\ncsvcut -c column_a,column_c data.csv > new.csv # select subset of columns"
  },
  {
    "objectID": "blog/whyblog/whyblog.html",
    "href": "blog/whyblog/whyblog.html",
    "title": "Why you should start a blog?",
    "section": "",
    "text": "My mentor, Sebastian Ruder advises PhD students to write blogs and has this to say: “Having a blog is the single thing that has led to the most positive interactions throughout my PhD.”1\nI was inspired, but I’ve been holding off writing blog posts consistently. I started in 2020 and stopped after two blog posts. Recently, I read a blog post by the co-founder of fast.ai, Rachel Thomas, “Why you should write blogpost”2 and watched an RStudio talk by David Robinson, “The unreasonable effectiveness of public work.”3 Both Rachel and David convinced me about the benefits of writing a blog post. Therefore, I braced up to start blogging consistently at least once fortnightly. As a Ph.D. student, writing habits will strongly build my writing and creativity muscles to the max.\nThere are many reasons one writes a blog. For me, below are only five reasons I think a blog will help me along the way of building my career."
  },
  {
    "objectID": "blog/whyblog/whyblog.html#deliberate-practice",
    "href": "blog/whyblog/whyblog.html#deliberate-practice",
    "title": "Why you should start a blog?",
    "section": "Deliberate practice",
    "text": "Deliberate practice\nDeliberate practice is a systematic, focused, consistent, goal-oriented training that builds expertise or improves performance.45 Building expertise in any field is not a marathon; it is a series of Sprints. Evidence has shown that experts or geniuses are always made, not born6 7. For example, bodybuilders, musicians, and footballers consistently practice to achieve mastery. No one becomes an expert from day one. The same is also true for writing and any other skills. Consistent writing, even small content but engaging and informative, will improve your writing skills. As we fondly say, “practice makes perfect.” Consistent practice allows one to do a task while thinking about other things. For example, a professional orator can deliver an excellent speech without reading from any single note. Stopping to think about the task can sometimes result in a flawless performance. People refer to this performance as being “in the zone. Aristotle said:”We are what we repeatedly do. Excellence, then, is not an act, but a habit.” \nDeliberate practice does not make what we learn easier; it changes the brain (Myelination). This concept is notably expressed as “cells that fire together, wire together.” Sometimes, we reach an “aha!” moment when learning difficult stuff - that is when someone has been struggling to understand a concept, and it suddenly becomes apparent - the clarity does not come out of nowhere.\n\nRather, it results from a steady accumulation of information. That’s because adding additional information opens up memories associated with the task. Once those memory neurons are active, they can form new connections. They also can form stronger connections within an existing network. Over time, your level of understanding increases until you suddenly “get” it 8.\n\nTherefore, this blog will serve as a way for me to do deliberate practices of many skills (writing, machine learning, visualization, python, r and, many more)"
  },
  {
    "objectID": "blog/whyblog/whyblog.html#repository-for-my-future-self",
    "href": "blog/whyblog/whyblog.html#repository-for-my-future-self",
    "title": "Why you should start a blog?",
    "section": "Repository for my future self",
    "text": "Repository for my future self\nI am absent-minded. I write code and forget how I did it or google the same thing many times So, anything that I often google or write a complex program, I will write a blog post on it. That way, I will refer to it. Hadley Wickham inspired me in his book R for Data Science; he said, if you write the same code three times, then, you write a function for that code. Hadley’s idea was adapted from code refactoring rule of thumb (Rule of three), which states that “two instances of similar code don’t require refactoring, but when similar code is used three times, it should be extracted into a new procedure.”9"
  },
  {
    "objectID": "blog/whyblog/whyblog.html#build-public-profile-and-network",
    "href": "blog/whyblog/whyblog.html#build-public-profile-and-network",
    "title": "Why you should start a blog?",
    "section": "Build public profile and network",
    "text": "Build public profile and network\nPutting your work or your skills to the public is a way to put your best foot forwards. Public work can be anything like Tweets, Blog post, GitHub Repo, or Book. Like-minded people with related interests may find your blog post, network with you, and give you feedback. Many opportunities may come in your future career from the network you build. An example of this was when David Robinson answered a question on Stack Overflow10, Stack Overflow engineer saw the brilliant answer and hired him (his first job at Stack Overflow). So, a blog allows one to showcase his skills, and other people can benefit from it.\n\n\n\n\n\nTweet transformed to a blog and book."
  },
  {
    "objectID": "blog/whyblog/whyblog.html#learning-by-teaching-protégé-effect",
    "href": "blog/whyblog/whyblog.html#learning-by-teaching-protégé-effect",
    "title": "Why you should start a blog?",
    "section": "Learning by teaching (protégé effect)",
    "text": "Learning by teaching (protégé effect)\nRobert Heinlein said, when one teaches, two learn. It means whenever you teach or explain a concept to someone, you will learn something from it or get a better insight and ultimately reach the “aha” moment. Therefore, writing a good tutorial about a brain-bending concept without dumbing it down is a great way to learn and increase visibility. As Einstein says, “If you can’t explain it simply, you don’t understand it well enough.” A study11 found that when students teach the lesson’s content (active learning), they develop a more in-depth and longer-lasting understanding of the material than students who do not teach it( passive learning). Therefore, this blog will allow me to write my research and summary of papers and man more. The approach of learning by teaching was widely known as Feynman learning technique12"
  },
  {
    "objectID": "blog/whyblog/whyblog.html#share-my-experience-and-opportunites",
    "href": "blog/whyblog/whyblog.html#share-my-experience-and-opportunites",
    "title": "Why you should start a blog?",
    "section": "Share my experience and opportunites:",
    "text": "Share my experience and opportunites:\nI naturally love to share my experience and other opportunities with people I know. Therefore, a blog post will serve as a way to share important resources that I come across and find useful. This will benefit a wider audience."
  },
  {
    "objectID": "blog/movingfilestodocker/movinfiles.html",
    "href": "blog/movingfilestodocker/movinfiles.html",
    "title": "Copying files to and from a docker containers",
    "section": "",
    "text": "You can use the docker cp command to copy files from the host to the container or vice versa.\n\n\nThe syntax for copying file from host to container is\ndocker cp [source] [container:]destination`.\nThe example below show how to copy a file (dataset_on_host.csv) from the host to a container folder called data. We saved the file with a new name (dataset_on_container.csv) in the container folder.\n\n\n\n\nThe syntax for copying file from container to host is:\ndocker cp [container:]source [destination]\n    \nThe example below also show how a file from a container is copied to the host.\n\nThank you for reading."
  },
  {
    "objectID": "blog/tensor/tensor.html",
    "href": "blog/tensor/tensor.html",
    "title": "Pytorch Tensor 101",
    "section": "",
    "text": "PyTorch is a Python-based open source and scientific computing package for building neural networks. It is dynamic graph-based framework that allows you to define your neural network in a way that is easy to understand and debug. Today, PyTorch is the most used deep learning framework and mostly use by researchers and engineers.\n\nPyTorch support GPU acceleration (making your code run faster) behind the scenes, better than NumPy. PyTorch also provides Autograd for automatic differentiation, which means that your code is automatically differentiated and you can use it to do backpropagation"
  },
  {
    "objectID": "blog/tensor/tensor.html#pytoch-installation",
    "href": "blog/tensor/tensor.html#pytoch-installation",
    "title": "Pytorch Tensor 101",
    "section": "Pytoch Installation",
    "text": "Pytoch Installation\nBefore you installed Pytorch, you need to install the following dependencies: Package Manager (e.g. pip, conda), Python, Numpy. For more information, please refer to the Pytorch documentation.\nFor me, I am using mac and conda as package manager, I therefore run the following command"
  },
  {
    "objectID": "blog/tensor/tensor.html#verification",
    "href": "blog/tensor/tensor.html#verification",
    "title": "Pytorch Tensor 101",
    "section": "VERIFICATION",
    "text": "VERIFICATION\nTo verify your installation works,\n\nimport torch\ntorch.manual_seed(1234)\ntorch.__version__\n\n'1.13.0.dev20220611'"
  },
  {
    "objectID": "blog/tensor/tensor.html#what-is-tensor",
    "href": "blog/tensor/tensor.html#what-is-tensor",
    "title": "Pytorch Tensor 101",
    "section": "What is Tensor",
    "text": "What is Tensor\n\nAssume we have 3 bedrooms, 1 carpark and 2 bathrooms. We can represent this data numerically in a form of vector [3, 1,2] to describe bedrooms, carpark and bathrooms\nTensor are the standard way of representing data in Pytorch, such as text, images, and audio. Their job is to represent data in a numerical way."
  },
  {
    "objectID": "blog/tensor/tensor.html#is-tensor-all-you-need",
    "href": "blog/tensor/tensor.html#is-tensor-all-you-need",
    "title": "Pytorch Tensor 101",
    "section": "is Tensor all you need?",
    "text": "is Tensor all you need?\n\nThere are many Python Data Structure for holding data including Python List and Numpy Array. List and Numpy Array operations are similar to Pytorch Tensor.\nLet us remember the basic of data structures in Python (List and Numpy Array) before we start using Pytorch Tensor"
  },
  {
    "objectID": "blog/tensor/tensor.html#from-python-lists-to-numpy-array",
    "href": "blog/tensor/tensor.html#from-python-lists-to-numpy-array",
    "title": "Pytorch Tensor 101",
    "section": "From Python lists to Numpy Array",
    "text": "From Python lists to Numpy Array\n\nPython does not have built-in support for Arrays, but Python Lists can be used instead.\nUsing our previous example, we can create a list of Python lists below.\n\n\na_list = [3, 1,2] #A list is the Python equivalent of an array\n\nprint(a_list) # print the list\nprint((type(a_list))) # print the type\nprint(a_list[0]) # subset the list\n\n[3, 1, 2]\n<class 'list'>\n3\n\n\n\nHowever, Python lists has the following limitations: It takes large memory size and slow.\n\n\nNumpy solved the problems with List:\n\nSize - Numpy data structures take up less space\nPerformance - they have a need for speed and are faster than lists\nFunctionality - SciPy and NumPy have optimized functions such as linear algebra operations built in.\n\n\n\nimport numpy as np\na_numpy = np.array([1,3,4]) # creating a numpy array\na_numpy\n\narray([1, 3, 4])\n\n\n\ntype(a_numpy) # nd arrays\n\nnumpy.ndarray\n\n\n\na_numpy[0] # we can subset similar to Python list\n\n1\n\n\n\na_numpy.shape # shape of the nd array\n\n(3,)\n\n\n\na_numpy.dtype # dtype of the nd array\n\ndtype('int64')\n\n\n\na_numpy.size # size of the nd array\n\n3\n\n\n\nPerformance comparison between Python lists and Numpy Arrays\n\nimport numpy as np\nimport time\n\n\nsize_of_vec = 1000\n\ndef pure_python_version():\n    t1 = time.time()\n    X = range(size_of_vec)\n    Y = range(size_of_vec)\n    Z = [X[i] + Y[i] for i in range(len(X)) ]\n    return time.time() - t1\n\ndef numpy_version():\n    t1 = time.time()\n    X = np.arange(size_of_vec)\n    Y = np.arange(size_of_vec)\n    Z = X + Y\n    return time.time() - t1\n\n\nt1 = pure_python_version()\nt2 = numpy_version()\nprint(t1, t2)\nprint(\"Numpy is in this example \" + str(t1/t2) + \" faster!\")\n\n0.00019288063049316406 0.0005578994750976562\nNumpy is in this example 0.3457264957264957 faster!"
  },
  {
    "objectID": "blog/tensor/tensor.html#from-numpy-arrays-to-torch-tensor",
    "href": "blog/tensor/tensor.html#from-numpy-arrays-to-torch-tensor",
    "title": "Pytorch Tensor 101",
    "section": "From Numpy Arrays to Torch Tensor",
    "text": "From Numpy Arrays to Torch Tensor\n\nTensors are like arrays, both are data structures that are used to store data. Tensor and Numpy arrays share common operations such as shape and size.\n\n\nTensors are generalization of vectors and matrices to an arbitrary number of dimensions.\n\n\n\nSimilar to how Numpy provides additional support not available in the Python list, so also Tensors provides support not available in Numpy array such as:\n\nGPU acceleration , which is a great advantage for deep learning,\ndistribute operations on multiple devices or machines,and\nkeep track of the graph of computations that created them ( usefull for backpropagation)."
  },
  {
    "objectID": "blog/tensor/tensor.html#let-us-learn-tensor",
    "href": "blog/tensor/tensor.html#let-us-learn-tensor",
    "title": "Pytorch Tensor 101",
    "section": "Let us Learn Tensor",
    "text": "Let us Learn Tensor\nVarious operations are available on tensors. In the next sections, we will discuss the following operations:\n\nCreating tensors.\nOperations with tensors.\nIndexing, slicing, and joining with tensors Computing gradients with tensors.\nUsing CUDA/MPS tensors with GPUs."
  },
  {
    "objectID": "blog/tensor/tensor.html#creating-tensors",
    "href": "blog/tensor/tensor.html#creating-tensors",
    "title": "Pytorch Tensor 101",
    "section": "Creating tensors",
    "text": "Creating tensors\n\nPyTorch allows us to create tensors in many different ways using the torch package. We will discuss some of these ways.\n\n\nCreating Random Tensor with a specific size\n\ntorch.tensor is a general Tensor constructor that infer the data type automatically.\n\n\nimport torch\n\na_random = torch.tensor((3,4)) # Create a random tensor\nprint(a_random)\n\ntensor([3, 4])\n\n\n\nprint(a_random.shape) # print the shape of the random tensor\nprint(a_random.size()) # print the size of the random tensor\nprint(type(a_random)) # print the type of the random tensor\nprint(a_random.type()) # print the type of the random tens\n\ntorch.Size([2])\ntorch.Size([2])\n<class 'torch.Tensor'>\ntorch.LongTensor\n\n\n\nNote: .shape is an alias for .size(), and was added to closely match numpy !\n\n\nIntead of allowing the torch.tensor to automatically determine the data type, you can explicitly specify the type of the data type by using the torch.type parameter\n\n\nimport torch\n\na_random = torch.tensor((3,4), dtype= torch.float) # Create a random tensor\nprint(a_random)\n\ntensor([3., 4.])\n\n\n\nprint(a_random.shape) # print the shape of the random tensor\nprint(a_random.size()) # print the size of the random tensor\nprint(type(a_random)) # print the type of the random tensor\nprint(a_random.type())\n\ntorch.Size([2])\ntorch.Size([2])\n<class 'torch.Tensor'>\ntorch.FloatTensor\n\n\n\nYou can also change an existing tensor type by using the\n\n\na_torch = torch.tensor([1, 2, 3]) \n\nprint(a_torch.type()) # Tensor type\n\ntorch.LongTensor\n\n\nWe can change from LongTensor t:\n\na_short =  a_torch.short() # Convert to short,  \na_float =  a_torch.float() # Convert to float()\n\nprint(a_short.type()) # Tensor type\nprint(a_float.type()) # Tensor type\n\ntorch.ShortTensor\ntorch.FloatTensor\n\n\n\nNote: A variant of torch.tensor constructor is torch.FloatTensorconstructor. When use, the default tensor type is FloatTensor. Infact, torch.Tensor is an alias for the torch.FloatTensor constructor.\n\n\nThe following two examples are equivalent:\n\n\na_random = torch.Tensor((3,4)) # Create a random tensor\nb_random = torch.FloatTensor((3,4)) # Create a random tensor\n\nprint(a_random.type())\nprint(b_random.type())\n\ntorch.FloatTensor\ntorch.FloatTensor\n\n\n\nI would recommend to stick to torch.tensor, if you would like to change the type, you can change\n\nTorch defines 10 tensor types with CPU and GPU variants: See different Pytorch Data Types:\n\nThe most common type (and generally the default) is torch.float32 or torch.float. This is referred to as “32-bit floating point”.\nBut there’s also 16-bit floating point (torch.float16 or torch.half) and 64-bit floating point (torch.float64 or torch.double).\nThe reason for all of these is to do with precision in computing. Precision is the amount of detail used to describe a number.\nThe higher the precision value (8, 16, 32), the more detail and hence data used to express a number.\nThis matters in deep learning and numerical computing because you’re making so many operations, the more detail you have to calculate on, the more compute you have to use.\n\n\nSo, lower precision datatypes are generally faster to compute on but sacrifice some performance on evaluation metrics like accuracy (faster to compute but less accurate).\n\n\n\n2: Creating Tensors from Random Numbers\nSimilar to the numpy, we can create a tensor from a random number.\n\na_random_torch = torch.randn(2, 3) # uniform random distribution numbers between 0 and 1\n# a_numpy_rand = np.random.randn(2,3) #numpy random normal distribution\n\nprint(a_random_torch)\n# print(a_numpy_rand)\n\ntensor([[ 0.0461,  0.4024, -1.0115],\n        [ 0.2167, -0.6123,  0.5036]])\n\n\n\na_random_torch = torch.rand(2, 3) # random normal distribution\n# a_numpy_rand = np.random.rand(2,3) \n\nprint(a_random_torch)\n# print(a_numpy_rand)\n\ntensor([[0.7749, 0.8208, 0.2793],\n        [0.6817, 0.2837, 0.6567]])\n\n\n\n\n3: Creating a filled tensor\n\na_same_scalar = torch.zeros(3,3)\nprint(a_same_scalar)\nprint(a_same_scalar.size())\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\ntorch.Size([3, 3])\n\n\n\ntorch.ones(3, 3) # torch.ones(size=(3, 3)) \n\ntensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]])\n\n\n\nAny PyTorch method with an underscore (_) refers to an in­place operation;\n\n\na_zero = torch.zeros(2, 3)\nprint(a_zero)\nprint(a_zero.fill_(5)) # inplace operation\nprint(a_zero)  # a_zero is now filled with 5\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\ntensor([[5., 5., 5.],\n        [5., 5., 5.]])\ntensor([[5., 5., 5.],\n        [5., 5., 5.]])\n\n\n###4: Creating and initializing a tensor from lists\n\na_list = torch.tensor([1, 2, 3])\na_list\n\ntensor([1, 2, 3])\n\n\n\n\n5: Creating and initializing a tensor from numpy arrays\n\nWe use torch.from_numpy to create a tensor from a numpy array.\n\n\nimport numpy as np\nnumpy_array = np.random.rand(2, 3) \nnumpy_array\n\ntorch_tensor = torch.from_numpy(numpy_array) # tensor from numpy array\ntorch_tensor\n\ntensor([[0.3487, 0.9072, 0.8480],\n        [0.7245, 0.6970, 0.4976]], dtype=torch.float64)\n\n\n\ntorch_tensor.type()\n\n'torch.DoubleTensor'\n\n\n\nThe datatype after creating of tensor from numpy array is DoubleTensor instead of the default FloatTensor. This corresponds with the data type of the NumPy random matrix, a float64,\n\n\nYou can always convert from PyTorch tensors to Numpy arrays using the numpy function torch.numpy().\n\n\ntorch_tensor.numpy()\n\narray([[0.3487288 , 0.90720583, 0.84795941],\n       [0.72447844, 0.69699952, 0.49759155]])\n\n\n\n\n6: Creating a range and tensors like\n\n# Use torch.arange(), torch.range() is deprecated \nzero_to_ten = torch.arange(0, 10) \nzero_to_ten\n\ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\n\nCreating tensor of type with the same shape as another tensor.\n\n# Can also create a tensor of zeros similar to another tensor\nten_zeros = torch.zeros_like(input=zero_to_ten) # will have same shape\nten_zeros\n\ntensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\n\n\nCreating Named Tensors\n\nNamed Tensors allow users to give explicit names to tensor dimensions.\nIn most cases, operations that take dimension parameters will accept dimension names, avoiding the need to track dimensions by position.\n\n\ntorch.zeros(2, 3, names=('N', 'C'))\n\n/var/folders/1h/b7ng0kgj3w78mg7n8k7q7rch0000gn/T/ipykernel_11570/697701580.py:1: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1654931446436/work/c10/core/TensorImpl.h:1489.)\n  torch.zeros(2, 3, names=('N', 'C'))\n\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.]], names=('N', 'C'))\n\n\n\nWe can use names to access tensor dimensions.\n\n\nimgs = torch.randn(1, 2, 2, 3 , names=('N', 'C', 'H', 'W')) \nimgs.names\n\n('N', 'C', 'H', 'W')\n\n\n\nimgs.names[0]\n\n'N'"
  },
  {
    "objectID": "blog/tensor/tensor.html#tensor-properties",
    "href": "blog/tensor/tensor.html#tensor-properties",
    "title": "Pytorch Tensor 101",
    "section": "Tensor properties",
    "text": "Tensor properties\nTensor has many properties including the following properties: the number of dimensions, the size, the type:\n\nTensor Dimensions\nWe can find the tensor dimensions using:ndim\n\n# Scalar\nscalar = torch.tensor(7)\nscalar\n\nscalar.ndim\n\n0\n\n\n\nMATRIX = torch.tensor([[1,2,3,4],\n                       [5,6,7,8]])\n\nMATRIX.ndim\n\n2\n\n\n\nYou can tell the number of dimensions a tensor in PyTorch has by the number of square brackets on the outside ([) and you only need to count one side of the brackets.\n\nIn practice, you’ll often see scalars and vectors denoted as lowercase letters such as y or a. And matrices and tensors denoted as uppercase letters such as X or W"
  },
  {
    "objectID": "blog/tensor/tensor.html#manipulating-tensors-tensor-operations",
    "href": "blog/tensor/tensor.html#manipulating-tensors-tensor-operations",
    "title": "Pytorch Tensor 101",
    "section": "Manipulating tensors (tensor operations)",
    "text": "Manipulating tensors (tensor operations)\n\nIn deep learning, data (images, text, video, audio, protein structures, etc) gets represented as tensors.\nA model learns by investigating those tensors and performing a series of operations (could be 1,000,000s+) on tensors to create a representation of the patterns in the input data.\nAfter you have created your tensors, you can operate on them like you would do with traditional programming language types, like +, ­, *, /.\n\n\nIndexing tensors\nIndexing and subsetting a tensor is similar to indexing a list.\n\nsome_list = list(range(6))\ntorch_list = torch.tensor(some_list)\ntorch_list\n\ntensor([0, 1, 2, 3, 4, 5])\n\n\n\nprint(torch_list[0]) # first element of the tensor\nprint(torch_list[1]) # second element of the tensor\n\ntensor(0)\ntensor(1)\n\n\n\ntorch_list[1:4] # subsetting a tensor\n\ntensor([1, 2, 3])\n\n\n\n\nTransposing Tensors\nTransposing 2D tensors is a simple operation using t\n\npoints = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\npoints\n\ntensor([[4., 1.],\n        [5., 3.],\n        [2., 1.]])\n\n\n\npoints_t = points.t()\npoints_t\n\ntensor([[4., 5., 2.],\n        [1., 3., 1.]])\n\n\nYou can also transpose 3D and higher tensors using the transpose method by specifying the two dimensions along which transposing (flipping shape and stride) should occur:\n\nsome_t = torch.ones(3, 4, 5)\ntranspose_t = some_t.transpose(0, 2)\nprint(some_t.shape)\nprint(transpose_t.shape)\n\ntorch.Size([3, 4, 5])\ntorch.Size([5, 4, 3])\n\n\n\n\nTensor View Operation\nTensor view operations returns a new tensor with the same data as the self tensor but of a different shape.\n\nx = torch.randn(2, 2)\nprint(x)\nprint(x.size())\n\ntensor([[-0.4790,  0.8539],\n        [-0.2285,  0.3081]])\ntorch.Size([2, 2])\n\n\n\ny = x.view(4)\nprint(y)\nprint(y.size())\n\ntensor([-0.4790,  0.8539, -0.2285,  0.3081])\ntorch.Size([4])\n\n\n\nUsing -1 in the shape argument will automatically infer the correct size of the dimension.\n\n\nz = x.view(-1, 2)  # the size -1 is inferred from other dimensions\n\nprint(z)\nprint(z.size())\n\ntensor([[-0.4790,  0.8539],\n        [-0.2285,  0.3081]])\ntorch.Size([2, 2])\n\n\n\nView Does not change tensor layout in memory, Transpose() operation change the tensor layout in memory.\n\n\n\nTensor Mathematical Basic Operations\nTensor addition is achive using torch.add as shown in the following example:\n\n# Create a tensor of values and add a number to it\ntensor = torch.tensor([1, 2, 3])\ntensor + 10\n\ntensor([11, 12, 13])\n\n\n\n# Multiply it by 10\ntensor * 10\n\ntensor([10, 20, 30])\n\n\n\n# Subtract and reassign\ntensor = tensor - 10\ntensor\n\ntensor([-9, -8, -7])\n\n\n\nPyTorch also has a bunch of built-in functions like torch.mul() (short for multiplcation) and torch.add() to perform basic operations.\n\n\n# Can also use torch functions\ntensor = torch.tensor([1, 2, 3])\ntorch.multiply(tensor, 10)  # multiply by 10\n\ntensor([10, 20, 30])\n\n\n\ntensor = torch.tensor([1, 2, 3])\n\ntorch.add(tensor, 20) # add by 20\n\ntensor([21, 22, 23])\n\n\n\ntorch.div(tensor, 20, rounding_mode='trunc') # divide by 20, with truncation as a rounding_mode\n\ntensor([0, 0, 0])\n\n\n\ntorch.div(tensor, 20, rounding_mode='floor') # divide by 20, with floor as a rounding_mode\n\ntensor([0, 0, 0])\n\n\n\ntorch.sum(tensor) # sum tensor entries  [1, 2, 3]\n\ntensor(6)"
  },
  {
    "objectID": "blog/tensor/tensor.html#matrix-multiplication-is-all-you-need",
    "href": "blog/tensor/tensor.html#matrix-multiplication-is-all-you-need",
    "title": "Pytorch Tensor 101",
    "section": "Matrix multiplication is all you need",
    "text": "Matrix multiplication is all you need\n\nIn deep learning algorithms (like neural networks), one of the most common operations is matrix multiplication.\nPyTorch implements matrix multiplication functionality in the torch.matmul() method.\nThe main two rules for matrix multiplication to remember are:\n\nThe inner dimensions must match:\n\n(3, 2) @ (3, 2) won’t work\n(2, 3) @ (3, 2) will work\n(3, 2) @ (2, 3) will work\n\nThe resulting matrix has the shape of the outer dimensions:\n\n(2, 3) @ (3, 2) -> (2, 2)\n(3, 2) @ (2, 3) -> (3, 3)\n\nNote: “@” in Python is the symbol for matrix multiplication.\nMore information about matrix multiplication can be found in the Matrix Multiplication section.\n\ntensor1 = torch.randn(3, 4)\ntensor2 = torch.randn(4)\n\nprint(tensor1.shape)\nprint(tensor2.shape)\n\ntorch.Size([3, 4])\ntorch.Size([4])\n\n\n\nresult = torch.matmul(tensor1, tensor2)\nresult.shape\n\ntorch.Size([3])\n\n\nNote: The difference between element-wise multiplication (multiply) and matrix multiplication (matmul) is the addition of values.\n\nmatmul: matrix multiplication\nmultiply: element-wise multiplication\n\n\ntensor = torch.tensor([1, 2, 3])\ntensor.shape\n\ntorch.Size([3])\n\n\nElement-wise matrix mutlication\n\ntensor * tensor\n\ntensor([1, 4, 9])\n\n\nMatrix multiplication\n\ntorch.matmul(tensor, tensor)\n\ntensor(14)\n\n\nCan also use the “@” symbol or torch.mm() for matrix multiplication, though not recommended\n\nprint(tensor @ tensor)\nprint(tensor.matmul(tensor))\n\ntensor(14)\ntensor(14)\n\n\n\nA matrix multiplication like this is also referred to as the dot product of two matrices. Neural networks are full of matrix multiplications and dot products.\n\nFor example, torch.nn.Linear() module (we’ll see this in action later on), also known as a feed-forward layer or fully connected layer, implements a matrix multiplication between an input x and a weights matrix A.\n\\[\ny = x\\cdot{A^T} + b\n\\]\nThank you for reading !"
  },
  {
    "objectID": "bookreading/Deep Work/daily0001.html",
    "href": "bookreading/Deep Work/daily0001.html",
    "title": "Deep Work",
    "section": "",
    "text": "I am currently reading Deep Work book by Cal NewPort"
  },
  {
    "objectID": "project copy/package/wizehiver/index.html",
    "href": "project copy/package/wizehiver/index.html",
    "title": "wizehiver",
    "section": "",
    "text": "Wizehiver Hex"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a PhD student at the University of Porto, Portugal. I am interested in low-resource natural language processing (NLP). I write stuff related to NLP and data science. You can reach out to me via shamsuddeen2004@gmail.com."
  },
  {
    "objectID": "dailyR/exercises_chap1/daily0001.html",
    "href": "dailyR/exercises_chap1/daily0001.html",
    "title": "R4DS Chapter 2 Exercises",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\nRun ggplot(data = mpg). What do you see?\n\n\nggplot(data = mpg)\n\n\n\n\nI see empty.\n\nHow many rows are in mpg? How many columns?\n\n\nnrow(mpg)\n\n[1] 234\n\n\nThere are 234 rows.\n\nWhat does the drv variable describe? Read the help for ?mpg to find out.\n\n\n?mpg\n\ndv means:\n\n“the type of drive train, where f = front-wheel drive, r = rear wheel drive, 4 = 4wd”\n\n\nMake a scatterplot of hwy vs cyl.\n\n\nggplot(data = mpg)+\ngeom_point(aes(hwy,cyl))\n\n\n\n\n\nWhat happens if you make a scatterplot of class vs drv? Why is the plot not useful?\n\n\nggplot(data = mpg)+\ngeom_point(aes(class,drv))\n\n\n\n\n\nMake a scatterplot of hwy vs cyl."
  },
  {
    "objectID": "til/dailyML01/daily0001.html",
    "href": "til/dailyML01/daily0001.html",
    "title": "Bias and Varience in Machine Learning Algorithms",
    "section": "",
    "text": "Machine learning algorithm prediction error can be classified into three types: irreducible error, bias error, and variable error."
  },
  {
    "objectID": "til/dailyML01/daily0001.html#bias-error",
    "href": "til/dailyML01/daily0001.html#bias-error",
    "title": "Bias and Varience in Machine Learning Algorithms",
    "section": "Bias error",
    "text": "Bias error\nA model bias is a prior assumption made by a model to make the target function easier to learn. For example, a linear algorithm assumes that the target function is linear. Hence, if the target function is non-linear, then the model will have a bias error. Though they are easy to learn and understand, they are not flexible and have lower predictive performance compared to complex or flexible algorithms. Therefore, less-flexible algorithms (e.g., linear regression, linear discriminant analysis, and logistic regression) have higher bias than complex or flexible algorithms (e.g., neural networks, decision trees, k-Nearest Neighbors, and support vector machines)."
  },
  {
    "objectID": "til/dailyML01/daily0001.html#variance-error",
    "href": "til/dailyML01/daily0001.html#variance-error",
    "title": "Bias and Varience in Machine Learning Algorithms",
    "section": "Variance error",
    "text": "Variance error\nAssume we train the same model with two different training datasets: N and M. Variance refers to the changes in the model when different training data sets are used. Certainly, the model will have a variance error when the training dataset is different. But, the error should not be too high between different training datasets.Therefore, low-variance algorithms have small changes when a different training dataset is used, and high-variance algorithms have large changes when a different training dataset is used. Flexible or complex algorithms have a high variance (e.g. SVM, Decision Trees, Neural Networks, k-Nearest Neighbors). However, non-flexible algorithms have a low variance (e.g., linear regression, linear discriminant analysis, and logistic regression).\nFinally, the goal of any supervised learning algorithm is to achieve low-bias and low-variance errors. Achieving these goals is the key to the success of any supervised learning algorithm. But, how can we achieve these goals? That is where the idea of Bias-Variance Tradeoff comes into play.\nSummary:\n\nNon-complex machine learning algorithms have high bias and low variance.\nNn-complex/flexible machine learning algorithms have a low bias but a high variance."
  },
  {
    "objectID": "til/labeled/daily0001.html",
    "href": "til/labeled/daily0001.html",
    "title": "labelled vs labeled",
    "section": "",
    "text": "I often forget which one to use between labeled and labelled.\nLabeled is an American spelling while labelled is British spelling.\nI use labelled since I write in British English."
  },
  {
    "objectID": "til/labeled/daily0001.html#focus-vs-focused",
    "href": "til/labeled/daily0001.html#focus-vs-focused",
    "title": "labelled vs labeled",
    "section": "Focus vs focused",
    "text": "Focus vs focused\nThere is a rule in the English language called the doubling up rule. Basically, if you add a vowel suffix to the end of a word, you typically want to double up the final letter to allow room for it.\nThe official requirements are that we ‘double a single consonant letter at the end of any base where the preceding vowel is spelled with a single letter and stressed’. For example: (bar, barring, barred), (beg, begging, begged), (occur, occurring, occurred).\nHowever, no doubling when the preceding vowel is unstressed. For example: panic becomes panicking, traffic becomes trafficking,\nNow is focus or focussing?\n\nThis word can take either double or single s, with the single option being highly preferred.\n\nfocusing/focussing and focused/focussed are all correct. !"
  },
  {
    "objectID": "til/dailyML01 copy/daily0001.html",
    "href": "til/dailyML01 copy/daily0001.html",
    "title": "Connecting two Indpendent Clauses with Comma and Semicolons",
    "section": "",
    "text": "To use a comma or not before “but” always trips me up. For future reference, I am documenting the general rules for connecting two indepå`endent clauses using coordinating conjunctions (e.g., but and is) and Independent Marker Words (e.g., however and also).\nLet us start with the general definition of what is clause and the two types of clauses."
  },
  {
    "objectID": "til/dailyML01 copy/daily0001.html#connecting-independent-clauses",
    "href": "til/dailyML01 copy/daily0001.html#connecting-independent-clauses",
    "title": "Connecting two Indpendent Clauses with Comma and Semicolons",
    "section": "Connecting independent clauses",
    "text": "Connecting independent clauses\nThere are two ways to connect independent clauses: using coordinating conjunctions and using conjunctive adverbs (connecting words).\n\nUsing coordinating conjunctions\nThere are seven types of coordinating conjunctions: and, but, for, or, nor, so, and yet. When two independent clauses are connected by a coordinating conjunction, the comma is necessary before the coordinating conjunction.\n\nI went to school today, but I forgot my bag at the airport.\nShe invited me to the party, but they all gave her excuses for not coming.\nToday is a rainy day, but I will go to the park to see him.\nThe beach is a lot of fun, yet the mountains are better.\n\n\n\nUsing conjunctive adverb or a transitional expression/connecting words\nAn independent marker word is a word (e.g., also, consequently, furthermore, however, moreover, nevertheless, and therefore.) that can be used to connect two independent clauses. If the second independent clause has an independent marker word, a semicolon is needed before the independent marker word.\n\nUse a semicolon to join two related independent clauses in place of a comma and a coordinating conjunction.\n\n\nI am going out; however, I’ll be home by nine.\nI am going to the store; however, I’ll be back in an hour.\nKathleen worked for many hours on all her homework; nevertheless, she was unable to finish all of it. (Conjunctive Adverb is used to connect two independent clauses. e.g nevertheless)\nHarvey is a good driver; moreover, he is a friendly one.\nTony finished reading three novels this week; in contrast, Joan finished only one novel (Transitional Expression: e.g In contrast, on the other hand)."
  },
  {
    "objectID": "til/dailyML01 copy/daily0001.html#common-mistakes-when-connecting-independent-clauses",
    "href": "til/dailyML01 copy/daily0001.html#common-mistakes-when-connecting-independent-clauses",
    "title": "Connecting two Indpendent Clauses with Comma and Semicolons",
    "section": "Common mistakes when connecting independent clauses",
    "text": "Common mistakes when connecting independent clauses\nSemicolons join ideas that are related and equal in grammatical structure. For example, the following is a wrong sentence:\n\nThis assignment is extra credit only; but we still need to hand it in.\n\nThe problem is that though the two clauses are related, we are not accurately connecting them. We cannot use a semicolon there because the clauses are joined by the coordinating conjunction but. We can use either the semicolon there or the conjunction, but not both. The following two options are correct:\n\nThis assignment is extra credit only, but we still need to hand it in.\n\n\nThis assignment is extra credit only; however, we still need to hand it in.\n\nAnother issue is that semicolons should not be used between a dependent clause and an independent clause.\n\nAlthough Nate is a kind employee; that new guy is not (incorrect). Although Nate is a kind employee, that new guy is not (correct).\n\nSometimes, you can replace the comma and coordinating conjunction with a semicolon since the connection between the two independent clauses is clear without the coordinating conjunction.\n\nJohn finished all his homework, but Kathleen did not finish hers (correct using coordinating conjunction and comma).\n\n\nJohn finished all his homework; Kathleen did not finish hers."
  },
  {
    "objectID": "til/dailyML01 copy/daily0001.html#semicolons-can-replace-commas",
    "href": "til/dailyML01 copy/daily0001.html#semicolons-can-replace-commas",
    "title": "Connecting two Indpendent Clauses with Comma and Semicolons",
    "section": "Semicolons can replace commas",
    "text": "Semicolons can replace commas\nUse a semicolon to replace a comma when you use a coordinating conjunction to link independent clauses that already contain commas.\nThe comma in the example below makes these independent clauses difficult to read because of the other commas in the clauses:\n\nMy dog is sick. She won’t eat, run around, or jump, nor will she go for a walk with me (incorrect).\n\nUsing a semicolon makes it easier to read the two independent clauses on either side of the coordinating conjunction:\n\nMy dog is sick. She won’t eat, run around, or jump; nor will she go for a walk with me.\n\nThank you for reading my note! Please feel free to contact me if you have any questions."
  },
  {
    "objectID": "til/git-rebasd/daily0001.html",
    "href": "til/git-rebasd/daily0001.html",
    "title": "Fatal: Not possible to fast-forward, aborting",
    "section": "",
    "text": "This happens is your branch is no longer directly based off of the branch you’re trying to merge it into - e.g. another commit was added to the destination branch that isn’t in your branch. Thus, you can’t fast-forward into it (because fast-forward requires your branch to completely contain the destination branch).\nSolution that works for me is from StackOverflow\ngit pull --rebase"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "This is my personal website. Content on this site is provided under a Creative Commons (CC-BY) 4.0 license. You may reuse this content as long as you indicate my authorship and provide a link back to the original material. Source code of the site is provided under the MIT license and may be reused without restriction."
  },
  {
    "objectID": "dailypython/py0001/py0001.html",
    "href": "dailypython/py0001/py0001.html",
    "title": "Getting and Knowing your Data",
    "section": "",
    "text": "url = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv'\n    \ndf = pd.read_csv(url, sep = '\\t')\n\nSee the first 10 entries\n\ndf.head(10)\n\n\n\n\n\n  \n    \n      \n      order_id\n      quantity\n      item_name\n      choice_description\n      item_price\n    \n  \n  \n    \n      0\n      1\n      1\n      Chips and Fresh Tomato Salsa\n      NaN\n      $2.39\n    \n    \n      1\n      1\n      1\n      Izze\n      [Clementine]\n      $3.39\n    \n    \n      2\n      1\n      1\n      Nantucket Nectar\n      [Apple]\n      $3.39\n    \n    \n      3\n      1\n      1\n      Chips and Tomatillo-Green Chili Salsa\n      NaN\n      $2.39\n    \n    \n      4\n      2\n      2\n      Chicken Bowl\n      [Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n      $16.98\n    \n    \n      5\n      3\n      1\n      Chicken Bowl\n      [Fresh Tomato Salsa (Mild), [Rice, Cheese, Sou...\n      $10.98\n    \n    \n      6\n      3\n      1\n      Side of Chips\n      NaN\n      $1.69\n    \n    \n      7\n      4\n      1\n      Steak Burrito\n      [Tomatillo Red Chili Salsa, [Fajita Vegetables...\n      $11.75\n    \n    \n      8\n      4\n      1\n      Steak Soft Tacos\n      [Tomatillo Green Chili Salsa, [Pinto Beans, Ch...\n      $9.25\n    \n    \n      9\n      5\n      1\n      Steak Burrito\n      [Fresh Tomato Salsa, [Rice, Black Beans, Pinto...\n      $9.25\n    \n  \n\n\n\n\n\nWhat is the number of observations(rows) dataset?\n\n# Solution 1\n\ncount_row = df.shape[0]  # Gives number of rows\ncount_col = df.shape[1]  # Gives number of columns\n\nprint(count_row, count_col)\n\n4622 5\n\n\n\nr, c = df.shape \n\nprint(r,c) # r = row and c = column\n\n4622 5\n\n\n\nlen() function is used to compute the length of each element in the Series/Index.\n\n\nlen(df) #   \n\n4622\n\n\n\nThe info() method prints information about the DataFrame. The information contains the number of columns, column labels, column data types, memory usage, range index, and the number of cells in each column (non-null values)\n\n\ndf.info() # inf \n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4622 entries, 0 to 4621\nData columns (total 5 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   order_id            4622 non-null   int64 \n 1   quantity            4622 non-null   int64 \n 2   item_name           4622 non-null   object\n 3   choice_description  3376 non-null   object\n 4   item_price          4622 non-null   object\ndtypes: int64(2), object(3)\nmemory usage: 180.7+ KB\n\n\n\nPrint the name of all the columns.\n\n\ndf.columns # list available columns in dataframe\n\nIndex(['order_id', 'quantity', 'item_name', 'choice_description',\n       'item_price'],\n      dtype='object')\n\n\n\n\nWhich was the most-ordered item?\n\nmost_ordered = df.groupby('item_name').sum().sort_values(['quantity'], ascending=False)\n\nmost_ordered.head()\n\n\n\n\n\n  \n    \n      \n      order_id\n      quantity\n    \n    \n      item_name\n      \n      \n    \n  \n  \n    \n      Chicken Bowl\n      713926\n      761\n    \n    \n      Chicken Burrito\n      497303\n      591\n    \n    \n      Chips and Guacamole\n      449959\n      506\n    \n    \n      Steak Burrito\n      328437\n      386\n    \n    \n      Canned Soft Drink\n      304753\n      351\n    \n  \n\n\n\n\n\n\nFor the most-ordered item, how many items were ordered?\n\ndf.groupby('item_name').sum().sort_values(['quantity'], ascending=False).head(1)\n\n\n\n\n\n  \n    \n      \n      order_id\n      quantity\n    \n    \n      item_name\n      \n      \n    \n  \n  \n    \n      Chicken Bowl\n      713926\n      761\n    \n  \n\n\n\n\n\n\nWhat was the most ordered item in the choice_description column?\n\ndf.groupby('choice_description').sum().sort_values(['quantity'], ascending=False).head(1)\n\n\n\n\n\n  \n    \n      \n      order_id\n      quantity\n    \n    \n      choice_description\n      \n      \n    \n  \n  \n    \n      [Diet Coke]\n      123455\n      159\n    \n  \n\n\n\n\n\n\nHow many items were orderd in total?\n\ndf.quantity.sum()\n\n4972\n\n\n\n\nTurn the item price into a float\n\ndf\n\n\n\n\n\n  \n    \n      \n      order_id\n      quantity\n      item_name\n      choice_description\n      item_price\n    \n  \n  \n    \n      0\n      1\n      1\n      Chips and Fresh Tomato Salsa\n      NaN\n      $2.39\n    \n    \n      1\n      1\n      1\n      Izze\n      [Clementine]\n      $3.39\n    \n    \n      2\n      1\n      1\n      Nantucket Nectar\n      [Apple]\n      $3.39\n    \n    \n      3\n      1\n      1\n      Chips and Tomatillo-Green Chili Salsa\n      NaN\n      $2.39\n    \n    \n      4\n      2\n      2\n      Chicken Bowl\n      [Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n      $16.98\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      4617\n      1833\n      1\n      Steak Burrito\n      [Fresh Tomato Salsa, [Rice, Black Beans, Sour ...\n      $11.75\n    \n    \n      4618\n      1833\n      1\n      Steak Burrito\n      [Fresh Tomato Salsa, [Rice, Sour Cream, Cheese...\n      $11.75\n    \n    \n      4619\n      1834\n      1\n      Chicken Salad Bowl\n      [Fresh Tomato Salsa, [Fajita Vegetables, Pinto...\n      $11.25\n    \n    \n      4620\n      1834\n      1\n      Chicken Salad Bowl\n      [Fresh Tomato Salsa, [Fajita Vegetables, Lettu...\n      $8.75\n    \n    \n      4621\n      1834\n      1\n      Chicken Salad Bowl\n      [Fresh Tomato Salsa, [Fajita Vegetables, Pinto...\n      $8.75\n    \n  \n\n4622 rows × 5 columns"
  }
]